End-to-End Medical Chatbot
==============================

A modular, production-ready medical chatbot system built with Flask, LangChain, and Pinecone for semantic search and retrieval from medical documents.

---

## Overview

This project answers medical questions using information extracted from large PDF medical documents. It leverages:
- **Flask** for the web interface and API
- **LangChain** for document processing and retrieval-augmented generation (RAG)
- **Pinecone** as a vector database for fast semantic search
- **HuggingFace** or **OpenAI** embeddings for vectorization

**Features:**
- Upload and index medical PDFs
- Fast, accurate retrieval of relevant information
- Modular codebase for easy extension
- Secure API key management via `.env`

---

## User Interface Interaction
The chat interface, built in HTML and served via Flask, acts as the user’s main entry point for interacting with the system in real time.

![Chatbot Web App Example](data/pics/acne_app.png)

If a user asks a question that is covered in the uploaded medical documents, the chatbot will retrieve and return a precise answer based on the original content.

![PDF Reference Example](data/pics/pdf_reference.png)

As shown above, the response includes information directly extracted from the source PDFs.

![Miscellaneous Diagram](data/pics/idk.png)

However, if a user asks a question unrelated to the documents, the chatbot will simply reply that it doesn't know, rather than generate a misleading or fabricated answer. It will avoid hallucinations by design.

---

## Architecture
![Chatbot Architecture Flow](data/pics/chatbot_architecture_flow.png)

This diagram illustrates the step-by-step flow of a user's query through the chatbot system. It starts from the user submitting a question via the web interface and ends with the Flask app displaying an answer generated by an LLM. The pipeline involves embedding the query, retrieving relevant content via Pinecone, forming a RAG (retrieval-augmented generation) prompt, and generating a response.

#### System Component Overview: Backend and Frontend
![Solution Architecture](data/pics/Solution_Architecture.png)
This diagram breaks down the entire system into backend and frontend components. It shows how medical PDFs are chunked and embedded to build a semantic index stored in Pinecone. On the frontend, user queries are embedded, matched with ranked document chunks, and passed to OpenAI to generate a final answer. The diagram visually separates the preprocessing and indexing phase from real-time inference.





---

## 📁 Project Structure

```
end-to-end-medical-chatbot/
├── app.py                # Flask web server and API
├── store_index.py        # Document loading, embedding, and Pinecone upsert
├── src/
│   ├── helper.py         # Helper functions (embedding, PDF loading, etc.)
│   └── prompt.py         # Prompt templates
├── data/                 # Place your PDF files here
├── templates/
│   └── chat.html         # Frontend UI
├── docs/                 # Sphinx documentation
├── .env                  # Environment variables (not committed)
├── README.md             # Project overview and usage
├── pyproject.toml        # Poetry configuration
├── poetry.lock           # Poetry lock file
├── notebooks/            # (Optional) Jupyter notebooks for experiments
└── reports/              # (Optional) Generated analysis and figures
```

---

## 👩‍💻 How to Use This Project (For New Users)

1. **Clone the repository**
    ```bash
    git clone https://github.com/boyangwan12/end-to-end-medical-chatbot.git
    cd end-to-end-medical-chatbot
    ```

2. **Install dependencies** (requires [Poetry](https://python-poetry.org/docs/#installation))
    ```bash
    poetry install
    ```

3. **Set up your environment variables**
    - Copy `.env.example` to `.env` (if provided) or create a new `.env` file:
      ```
      PINECONE_API_KEY=your-pinecone-key
      OPENAI_API_KEY=your-openai-key  # if using OpenAI embeddings
      ```

4. **Add your medical PDFs**
    - Place your PDF files in the `data/` directory.

5. **Index your documents**
    ```bash
    poetry run python store_index.py
    ```
    - This will split your PDFs, generate embeddings, and upsert them to Pinecone.

6. **Run the chatbot web app**
    ```bash
    poetry run python app.py
    ```
    - Open [http://127.0.0.1:8080/](http://127.0.0.1:8080/) in your browser to access the chatbot UI.

7. **Ask questions!**
    - Type a medical question in the chat. The app will retrieve relevant info from your PDFs and generate an answer using an LLM.


---

**Tips:**
- For development, edit code in `src/` and rerun the relevant scripts.
- Keep your API keys safe! Never commit `.env` to version control.
