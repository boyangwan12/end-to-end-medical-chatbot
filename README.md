End-to-End Medical Chatbot
==============================

A modular, production-ready medical chatbot system built with Flask, LangChain, and Pinecone for semantic search and retrieval from medical documents.

---

## Overview

This project answers medical questions using information extracted from large PDF medical documents. It leverages:
- **Flask** for the web interface and API
- **LangChain** for document processing and retrieval-augmented generation (RAG)
- **Pinecone** as a vector database for fast semantic search
- **HuggingFace** or **OpenAI** embeddings for vectorization

**Features:**
- Upload and index medical PDFs
- Fast, accurate retrieval of relevant information
- Modular codebase for easy extension
- Secure API key management via `.env`
---
## Architecture
#### End-to-End Medical Chatbot Architecture Flow
![Chatbot Architecture Flow](data/pics/chatbot_architecture_flow.png)
This diagram illustrates the step-by-step flow of a user's query through the chatbot system. It starts from the user submitting a question via the web interface and ends with the Flask app displaying an answer generated by an LLM. The pipeline involves embedding the query, retrieving relevant content via Pinecone, forming a RAG (retrieval-augmented generation) prompt, and generating a response.

#### System Component Overview: Backend and Frontend
![Solution Architecture](data/pics/Solution_Architecture.png)
This diagram breaks down the entire system into backend and frontend components. It shows how medical PDFs are chunked and embedded to build a semantic index stored in Pinecone. On the frontend, user queries are embedded, matched with ranked document chunks, and passed to OpenAI to generate a final answer. The diagram visually separates the preprocessing and indexing phase from real-time inference.

## Application Explaination

To better understand the system, follow this walkthrough using diagrams as visual cues:

## Step 1: End-to-End Architecture Overview
The solution starts when a user submits a medical question. The Flask backend handles the query, routes it through the embedding model, retrieves relevant chunks from Pinecone, constructs a prompt, and passes it to an LLM for response generation. The process concludes by sending the generated response back to the user interface.

**See Image:**


## Step 2: User Interface Interaction
The chat interface (built in HTML and served via Flask) is the userâ€™s entry point. It allows real-time interaction with the system.

**Refer to Image:**
![Chatbot Web App Example](data/pics/acne_app.png)

## Step 3: Internal Query Flow
After the question is received, the backend:
- Embeds the query.
- Queries Pinecone for similar document chunks.
- Combines those chunks with the query into a RAG prompt.
- Sends the prompt to the LLM.
- Returns the generated answer to the UI.

**Image:**


## Step 4: Document Indexing Pipeline
Before the system can answer anything, PDFs must be prepared. The `store_index.py` script loads each medical PDF, splits it into overlapping chunks, converts them into vectors, and stores them in Pinecone.

**Visualized in Image:**
![PDF Reference Example](data/pics/pdf_reference.png)

## Step 5: Development & Debugging
During development, logs, error cases, and edge flows were handled using test diagrams and internal state visualizations.

**See Image:**
![Miscellaneous Diagram](data/pics/idk.png)



---

## ğŸ“ Project Structure

```
end-to-end-medical-chatbot/
â”œâ”€â”€ app.py                # Flask web server and API
â”œâ”€â”€ store_index.py        # Document loading, embedding, and Pinecone upsert
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ helper.py         # Helper functions (embedding, PDF loading, etc.)
â”‚   â””â”€â”€ prompt.py         # Prompt templates
â”œâ”€â”€ data/                 # Place your PDF files here
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ chat.html         # Frontend UI
â”œâ”€â”€ docs/                 # Sphinx documentation
â”œâ”€â”€ .env                  # Environment variables (not committed)
â”œâ”€â”€ README.md             # Project overview and usage
â”œâ”€â”€ pyproject.toml        # Poetry configuration
â”œâ”€â”€ poetry.lock           # Poetry lock file
â”œâ”€â”€ notebooks/            # (Optional) Jupyter notebooks for experiments
â””â”€â”€ reports/              # (Optional) Generated analysis and figures
```

---

## ğŸ‘©â€ğŸ’» How to Use This Project (For New Users)

1. **Clone the repository**
    ```bash
    git clone https://github.com/boyangwan12/end-to-end-medical-chatbot.git
    cd end-to-end-medical-chatbot
    ```

2. **Install dependencies** (requires [Poetry](https://python-poetry.org/docs/#installation))
    ```bash
    poetry install
    ```

3. **Set up your environment variables**
    - Copy `.env.example` to `.env` (if provided) or create a new `.env` file:
      ```
      PINECONE_API_KEY=your-pinecone-key
      OPENAI_API_KEY=your-openai-key  # if using OpenAI embeddings
      ```

4. **Add your medical PDFs**
    - Place your PDF files in the `data/` directory.

5. **Index your documents**
    ```bash
    poetry run python store_index.py
    ```
    - This will split your PDFs, generate embeddings, and upsert them to Pinecone.

6. **Run the chatbot web app**
    ```bash
    poetry run python app.py
    ```
    - Open [http://127.0.0.1:8080/](http://127.0.0.1:8080/) in your browser to access the chatbot UI.

7. **Ask questions!**
    - Type a medical question in the chat. The app will retrieve relevant info from your PDFs and generate an answer using an LLM.


---

**Tips:**
- For development, edit code in `src/` and rerun the relevant scripts.
- Keep your API keys safe! Never commit `.env` to version control.
